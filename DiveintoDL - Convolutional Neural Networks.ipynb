{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Convolutional Neural Networks\n",
    "\n",
    "### 6.1 From Dense Layers to Convolutions\n",
    "\n",
    "The models that were discussed in previous chapters are fine options if yopu are dealing with tabular data. By tabular we mean that the data consists of rows corresponding to examples and columns corresponding to features. With tabular data, we might anticipate that pattern we seek could require modeling interactions among the features, but do not assume anything a priori about which features are related to each other or in what way.\n",
    "\n",
    "Sometimes we truly may not have any knowledge to guide the construction of more cleverly-organized architectures. In these cases, a multilayer perceptron is often the best that we can do. However, once we start dealing with high-dimensional perceptual data, these *structure-less* networks can grow unwieldy.\n",
    "\n",
    "For instance, let's return to our running example of distinguishing cats from dogs. Say that we do a thorough job in data collection, collecting an annotated sets of high-quality 1-megapixel photographs. This means that the input into a network has *1 million dimensions* (1-megapixel is 1 million pixels). Even an aggresive reduction to *1,000 hidden dimensions* would require a *dense* (fully-connected) layer to support $10^9$ parameters. Unless we have an extremely large dataset (perhaps billions of photos), lots of GPUs, a talent for extreme distributed optimization, and an extraordinary amount of patience, learning the parameters of this network may turn out to be impossible. \n",
    "\n",
    "A careful reader might object to this argument on the basis that 1 megapixel resolution may not be necessary. However, while you could get away with 100,000 pixels, we grossly underestimated the number of hidden nodes that it typically takes to learn good hidden representations of images. Learning a binary classifier with so many parameters might seem to require that we collect an enormous dataset, perhaps comparable to the number of dogs and cats on the planet. And yet both humans and computers are able to distinguish cats from dogs quite well, seemingly contradicting these conclusions. That is because images exhibit rich structure that is typically exploited by humans and machine learning models alike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Invariances\n",
    "\n",
    "Imagine that you want to detect an object in an image. It seems reasonable that whatever method we use to recognize objects should not be overly concerned **with the precise *location* of the object**. Ideally, we could learn a system that would somehow exploit this knowledge. Pigs usually don't fly and planes usually do not swim. Nonetheless, we could still recognize a flying pig were one to appear. This idea is taken to the extreme in the children's game \"Where's Waldo\". The game consists of a number of chaotic scenes bursting with activity and Waldo shows up somewhere in each (typically lurking in some unlikely location). The reader's goal is to locate him. Despite his characteristic outfit, this can be surprisingly difficult, due to the large number of **confounders** \n",
    "\n",
    "Back to images, the intuitions we have been discussing could be made more concrete yielding a few key principles for building neural networks for computer vision:\n",
    "\n",
    "- 1) Our vision systems should, in some sense, respond similarly to the same object **regardless of where it appears in the image** (*translation invariance*)\n",
    "- 2) Our vision system should, in some sense, focus on **local regions, without regard for what else is happening in the image at greater distances** (*locality*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Constraining the MLP\n",
    "\n",
    "To start off, let's consider what an MLP would look like with $h \\times w$ images as inputs (represented as matrices in math, and as 2D arrays in code), and hidden representations similarly organized as $h \\times w$ / 2D arrays. Let $x[i, j]$ and $h[i, j]$ denote pixel location $(i, j)$ in an image and hidden representation, respectively. Consequently, to have each of the $hw$ hidden nodes receive input from each of the $hw$ inputs, we would switch from using weight matrices (as we did previously in MLPs) to representing our parameters as four-dimensional weight tensors.\n",
    "\n",
    "We could formally express this dense layer as follows:\n",
    "\n",
    "$h[i, j] = u[i, j] + \\sum{W[i,j,k,l]} * x[k, l] = u[i, j] + \\sum{V[i,j,a,b]} * x[i+a, j + b]$\n",
    "\n",
    "**The switch from W to V is entirely cosmetic (for now) since there is a one-to-one correspondence between coefficients in both tensors**. We simply re-index the subscripts $(k, l)$ such that $k = i + a$ and $l = j + b$. In other words, we set $V[i,j,a,b] = W[i, j, i + a, j + b]$. **The indices $a,b$ run over both positive and negative offsets, covering the entire image. For any given location $(i, j)$ in the hidden layer $h[i, j]$, we compute its value by summing over pixels in $x$, centered around $(i, j)$ and weighted $V[i,j,a,b]$**.\n",
    "\n",
    "Now, let's invoke the first principle we established above: *translation invariance*. This implies that **a shift in the inputs $x$ should simply lead to a shift in activations $h$**. This is only possible if $V$ and $u$ **DO NOT** actually depend on $(i, j)$, i.e., we have $V[i,j,a,b] = V[a,b]$ and $u$ is a constant. As a result we can simplify the definition for $h$.\n",
    "\n",
    "$h[i,j] = u + \\sum{V[a,b]} * x[i + a, j + b]$\n",
    "\n",
    "This is a convolution! **We are effectively weighting pixels $(i + a, j + b)$ in the VICINITY of $(i, j)$ with coefficients $V[a,b]$ to obtain the value $h[i,j]$. Note that V[i,j] needs many fewer coefficients than $V[i,j,a,b]$. For a 1 megapixel image it has at most 1 million coefficients**. This is 1 million fewer parameters since it no longer depends on the location within the image. We have made significant progress!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3 Convolutions\n",
    "\n",
    "Let's briefly review why the above operation is called a *convolution*. In mathematics, the convolution between two functions, say $f, g: \\mathbf{R}^d \\rightarrow R$ is defined as:\n",
    "\n",
    "$[f\\circledast{g}](x) = \\int_{R^d}f(z)g(x-z)dz$\n",
    "\n",
    "That is, we measure the overlap between f and g when **both functions are shifted by $x$ and \"flipped**. Whenever we have discrete objects, the integral turns into a sum. For instance, for vectors defined on $l_2$, i.e., the set of square summable infinite dimensional vectors with index running over $Z$ we obtain the following definition.\n",
    "\n",
    "$[f\\circledast{g}](i) = \\sum_{a}f(a)g(i-a)$\n",
    "\n",
    "\n",
    "For two-dimensional arrays, we have a corresponding sum with indices $(i, j)$ for $f$ and $(i - a, j - b)$ for $g$ respectively. This looks similar to the definition above, with one major difference. Rather than using $(i + a, j + b)$, we are using the difference instead. Note, though, that this distinction is mostly cosmetic since we can always match the notation by using $V[a,b] = V[-a, -b]$ to obtain $h = x \\circledast V$. Also note that the original definition is actually a *cross correlation*. We will come back to this in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.4 Waldo Revisited\n",
    "\n",
    "Let's see what this looks like if we want to build an improved Waldo detector. The convolutional layer picks windows of a given size and weighs intensities **according to the mask V**. **We expect that wherever the \"waldoness\" is highest, we will also find a peak in the hidden layer activations**\n",
    "\n",
    "There is just a problem with this approach: so far we blissfully ignored that images consist of 3 channels: red, green, and blue. In reality, images aren't quite two-dimensional objects but **rather a $3^{rd}$ order tensor**, e.g., with shape $1024 \\times 1024 \\times 3$ pixels. Only two of these axes concern spatial relationships, while the $3^{rd}$ can be regarded as assigning a multidimensional representation to *each pixel location*.\n",
    "\n",
    "We thus index $\\textbf{x}$ as $x[i, j, k]$. The convolutional mask has to adapt accordingly. Instead of $V[a, b]$ we now have $V[a, b, c]$\n",
    "\n",
    "Moreover, just as our input consists of a $3^{rd}$ order tensor, it turns out to be a good idea to similarly formulate our hidden representations as $3^{rd}$ order tensors. In other words, rather than just having a 1D representation corresponding to each spatial location, we want to have a multidimensional hidden representations corresponding to each spatial location. We could think of the hidden representation as comprising a number of 2D grids stacked on top of each other. These are sometimes called *channels* or *feature maps*. Intuitively you might imagine that at lower layers, some channels specialize at recognizing edges for example. We can take care of this by adding a fourth coordinate to $V$ via $V[a, b, c, d]$.\n",
    "\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- Translation invariance in images implies that all patches of an image will be treated in the same manner\n",
    "\n",
    "- Locality means that only a small neighborhood of pixels will be used for computation\n",
    "\n",
    "- Channels on input and output allow for meaningful feature analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Convolutions for Images\n",
    "\n",
    "#### 6.2.1 The Cross-Correlation Operator\n",
    "\n",
    "In a convolutional layer, an **input array** and a **correlation kernel array** are combined to produce an output array through a cross-correlation operation. Let's see how this works for two dimensions. The input in our example is a 2D array with a height of 3 and width of 3. We mark the shape of the array as $3 \\times 3$ or (3,3). The height and width of the kernel array are both 2. Common names for this array in the deep learning community include *kernel* and *filter*. The shape of the kernel window (also known as the convolutional window) is given precisely by the height and width of the kernel (here it is $2 \\times 2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    '''Compute 2D cross-correlation.'''\n",
    "    h, w = K.shape\n",
    "    Y = np.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,1,2], \n",
    "              [3,4,5], \n",
    "              [6,7,8]])\n",
    "K = np.array([[0, 1], \n",
    "              [2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19., 25.],\n",
       "       [37., 43.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Convolutional Layers\n",
    "\n",
    "A convolutional layer **cross-correlates** the input and kernels and adds a scalar bias to produce an output. The parameters of the convolutional layer are precisely the values that constitute the kernel and the scalar bias. **When training the models based on convolutional layers, we typically initialize the kernels randomly, just as we would with a fully-connected layer.**\n",
    "\n",
    "We are now ready to implement a 2D-convolutional layer based on the corr2d function defined above. In the `__init__` constructor function, we declare weight and bias as the two model parameters. The forward computation function *forward* calls the corr2d function and adds the bias. As with $h \\times w$ cross-correlation we also refer to convolutional layers as $h \\times w$ convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Block):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        super(Conv2D, self).__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape = kernel_size)\n",
    "        self.bias = self.params.get('bias', shape = (1,))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight.data()) + self.bias.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Object Edge Detection in Images\n",
    "\n",
    "Let's look at a simple application of a convolutional layer: detecting the edge of an object in an image **by finding the location of the pixel change**. First, we construct an 'image' of $6 \\times 8$ pixels. The middle four columns are black (0) and the rest are white (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct a kernel K with a height of 1 and width of 2. When we perform the cross-correlation operation with the input, ***if the horizontally adjacent elements are the same, the output is zero***. Otherwise, the output is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([[1, -1]])\n",
    "K_2 = np.array([[1],\n",
    "                [-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter X and our designed kernel K to perform the cross-correlation operations. As you can see, we will detect 1 for the edge from white to black and -1 for the edge from black to white. The rest of the outputs are 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the kernel to the transposed image. As expected, it vanishes. **The kernel K only detects vertical edges.**\n",
    "\n",
    "\n",
    "*Note*: What would happen if I were to transpose the kernel so that it is represented as a column vector? (2 x 1 in this case)\n",
    "\n",
    "Answer: It would work correctly, i.e. the result would be the transpose of Y (that is calculated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1., -1., -1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.T, K_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.4 Learning a Kernel\n",
    "\n",
    "Designing an edge detector by finite differences [1, -1] is neat if we know this is precisely what we are looking for. However, as we look at larger kernels, and consider successive layers of convolutions, it might be impossible to specify precisely what each filter should be doing manually. \n",
    "\n",
    "Now, let's see whether we can **learn the kernel** that generated Y from X by looking at the (input, output) pairs only. We first construct a convolutional layer and initialize its kernel as a random array. Next, in each iteration, we will use the **squared error to compare Y and the output of the convolutional layer, then calculate the gradient to update the weight**. For the sake of simplicity, in this convolutional layer, we will ignore the bias.\n",
    "\n",
    "We previously constructed the Conv2D class. However, since we used single-element assignments, Gluon has some trouble finding the gradient. Instead, we use the built-in Conv2D class provided by Gluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a convolutional layer with 1 output channel\n",
    "# (channels will be introduced in the following sections)\n",
    "# and a kernel array shape of (1,2)\n",
    "\n",
    "conv2d = nn.Conv2D(1, kernel_size=(1, 2))\n",
    "\n",
    "conv2d.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two dimensional convolutional layer **uses four-dimensional input** and output in the format of: \n",
    "\n",
    "- (example channel, height, width)\n",
    "\n",
    "where the batch size and the number of channels are both 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(1, 1, 6, 8)\n",
    "Y = Y.reshape(1, 1, 6, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:2, loss: 5.0633144\n",
      "batch:4, loss: 0.86372524\n",
      "batch:6, loss: 0.15074256\n",
      "batch:8, loss: 0.027679807\n",
      "batch:10, loss: 0.005622587\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    with autograd.record():\n",
    "        Y_hat = conv2d(X)\n",
    "        l = (Y_hat - Y) ** 2\n",
    "    l.backward()\n",
    "    # Ignoring bias, for the sake of simplicity\n",
    "    conv2d.weight.data()[:] -= 3e-2 * conv2d.weight.grad()\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'batch:{i + 1}, loss: {l.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the error has dropped to a small value after 10 iterations. Now, we will take a look at the kernel array we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9925716, -0.9841616]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data().reshape(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.5 Cross-Correlation and Convolution\n",
    "\n",
    "Recall the observation from the previous section that **cross-correlation and convolution are equivalent**. In the figure above it is easy to see this correspondence. Simply flip the kernel from the bottom left to the top right. In this case the indexing in the sum is reverted, yet the same result can be obtained. In keeping with standard terminology with deep learning literature, we will continue to refer to the cross-correlation operation as a convolution even though, strictly speaking, it is slightly different.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- The core computation of a two-dimensional convolutional layer is a two-dimensional cross correlation operation. In its simplest form, this performs a cross-correlation on the two-dimensional input data and the kernel, and then adds a bias.\n",
    "\n",
    "- We can design a kernel to detect edges in images\n",
    "\n",
    "- We can learn the kernel through data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Padding and Stride\n",
    "\n",
    "In the previous example, our input had a height and width of 3 and a convolution kernel with a height and width of 2, yielding an output with a height and a width of 2. In general, assuming the input shape is $n_h \\times n_w$ and the convolution kernel window shape is $k_h \\times k_w$, then the output shape will be\n",
    "\n",
    "- $(n_h - k_h + 1) \\times (n_w - k_w + 1)$\n",
    "\n",
    "Therefore, the output shape of the convolutional layer is **determined by the shape of the input and the shape of the convolution kernel window**.\n",
    "\n",
    "In several cases we might want to incorporate particular techniques - **padding and strides**, regarding the size of the output:\n",
    "\n",
    "- In general, since kernels generally have width and height greater than 1, that means that after applying many successive convolutions, we will wind up with an output that is much smaller than our input. If we start with a $240 \\times 240$ pixel image, 10 layers of $5 \\times 5$ convolutions reduce the image to $200 \\times 200$ pixels, slicing off 30% of the original image and with it obliterating any interesting information on the boundaries of the original image. *Padding* handles this issue.\n",
    "- In some cases, we want to reduce the resolution drastically if, say, we find our original input resolution to be unwieldy. *Strides* can help in these instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 Padding\n",
    "\n",
    "As described above, one tricky issue when applying convolutional layers is that we're losing pixels on the perimeter of our image. Since we typically use small kernels, for any given convolution, we might only lose a few pixels, but this can add up as we apply many successive convolutional layers. One straightforward solution to this problem is to add extra pixels to fill the boundary around our input image, thus increasing the effective size of the image. Typically, we set the values of extra pixels to 0.\n",
    "\n",
    "In general, if we add a total of $p_h$ rows of padding (roughly half on top and half on bottom) and a total of $p_w$ columns of padding (roughly half on the left and half on the right), the output shape will be\n",
    "\n",
    "- $(n_h - k_h + p_h + 1) \\times (n_w - k_w + p_w + 1)$\n",
    "\n",
    "This means that the height and width of the output will increase by $p_h$ and $p_w$ respecively.\n",
    "\n",
    "**In many cases, we will want to set $p_h = k_h - 1$ and $p_w = k_w - 1$ to give the input and output the same height and width**. This will make it easier to predict the output shape of each layer when constructing the network. Assuming that $k_h$ is odd here, we will pad $\\frac{p_h}{2}$ rows on both sides of the height. If $k_h$ is even, one possibility is to pad [$p_h$ / 2] rows on the top of the input and [$p_h$ / 2] rows on the bottom. We will pad both sides of the width the same way.\n",
    "\n",
    "Convolutional neural networks commonly use convolutional kernels with odd height and width values, such as 1, 3, 5, or 7. **Choosing odd kernel sizes has the benefit that we can preserve the spatial dimensionality while padding with the same number of rows on top and bottom, and the same number of columns on left and right.**\n",
    "\n",
    "Moreover, this practice of using odd kernels and padding to precisely preserve dimensionality offers a clerical benefit. For any two-dimensional array X, when the kernel size is odd and the number of padding rows and columns on all sides are the same, producing an output with the same height and width as the input, we know that the output Y[i,j] is calculated by cross-correlation of the input and convolution kernel with the window centered on X[i,j].\n",
    "\n",
    "In the following example, we create a 2D convolutional layer with a height and width of 3 and apply 1 pixel of padding on all sides. Given an input with a height and width of 8, we find that the height and width of the output is also 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, we define a function to calculate the convolutional layer.\n",
    "# This function initializes the convolutional layer weights and performs\n",
    "# corresponding dimensionality elevations and reductions on the input and output.\n",
    "def comp_conv2d(conv2d, X):\n",
    "    conv2d.initialize()\n",
    "    # (1, 1) indicates that the batch size and the number of channels are both 1\n",
    "    X = X.reshape((1,1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # Exclude the first two dimensions (batch size and no of channels) as they are\n",
    "    # not relevant to us here\n",
    "    return Y.reshape(Y.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that here 1 row or column is padded on either side, so a total of 2\n",
    "# rows or columns are added\n",
    "conv2d = nn.Conv2D(1, kernel_size=3, padding=1)\n",
    "X = np.random.uniform(size = (8, 8))\n",
    "\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the height and width of the convolution kernel are different, we can make the output and input have the same height and width by setting different padding numbers for height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we use a convolution kernel with a height of 5 and a width of 3. The\n",
    "# padding numbers on both sides of the height and width are 2 and 1,\n",
    "# respectively\n",
    "conv2d = nn.Conv2D(1, kernel_size=(5,3), padding = (2,1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Stride\n",
    "\n",
    "When computing cross-correlation, we start with the convolution window at the top-left corner of the input array, and then slide it over all locations both down and to the right. In previous examples, we default to sliding one pixel at a time. However, sometimes, either for computational efficiency or because we wish to downsample, we move our window more than one pixel at a time, skipping the intermediate locations.\n",
    "\n",
    "We refer to the number of rows and columns traversed per slide as the *stride* (AIN'T NOTHING GONNA BREAK MY STRIDE). So far, we have used strides of 1, both for height and width. Sometimes, we may want to use a larger stride. In the example in the book, the figure shows a two-dimensional cross-correlation operation with a stride of 3 vertically and 2 horizontally. **We can see that when the second element of the first column is output, the convolution window slides down three rows. The convolution window slides two columns to the right when the second element of the first row is output**. When the convolution window slides three columns to the right on the input, there is no output because the input element cannot fill the window (unless we add another column of padding).\n",
    "\n",
    "In general, when the stride for the height is $s_h$ and the stride for the width is $s_w$, the output shape is\n",
    "\n",
    "$\\frac{(n_h - k_h + p_h + s_h)}{s_h} \\times \\frac{(n_w - k_w + p_w + s_w}{s_w}$ \n",
    "\n",
    "If we set $p_h = k_h - 1$ and $p_w = k_w - 1$, then the output shape will be simplified to $\\frac{(n_h + s_h - 1)}{s_h} \\times \\frac{(n_w + s_w - 1)}{s_w}$. Going a step further, if the input height and width are divisible by the strides on the height and width, then the output shape will be $(n_h/s_h) \\times (n_w/s_w)$\n",
    "\n",
    "Below, we set the strides on both the height and width to 2, thus halving the input height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly more complicated example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2D(1, kernel_size=(3,5), padding=(0,1), strides = (3,4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of brevity, when the padding number on both sides of the input height and width are $p_h$ and $p_w$ respectively, we call the padding $(p_h, p_w)$. Specifically, when $p_h = p_w = p$, the padding is $p$. When the strides on the height and width are $s_h$ and $s_w$, respectively, we call the stride $(s_h, s_w)$. Specifically, when $s_h = s_w = s$, the stride is $s$. By default, the padding is 0 and the stride is 1. In practice, **we rarely use inhomogeneous strides or padding**, i.e., we usually have $p_h = p_w$ and $s_h = s_w$.\n",
    "\n",
    "### Recap\n",
    "\n",
    "- Padding can increase the height and width of the output. This is often used to give the output the same height and width as the input.\n",
    "- The stride can reduce the resolution of the output, for example reducing the height and width of the output to only 1 / n of the height and width of the input (n is an integer greater than 1).\n",
    "- Padding and stride can be used to adjust the dimensionality of the data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Multiple Input and Output Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
